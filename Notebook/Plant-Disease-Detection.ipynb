{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298f03e-03d1-412b-b661-770c7a1f0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a9397-05c0-4228-ac8d-63dd5e0eca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = './new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n",
    "VALIDATION_DIR = './new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7b210-5a71-4b02-b8ef-ff0a985cef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Diseases_classes = os.listdir(TRAIN_DIR)\n",
    "print(\"\\nTotal number of classes are: \", len(Diseases_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198efdd-11bf-4110-9dc0-aeec46c86c14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_for_invalid_files(dataset_dir):\n",
    "    # List to store paths of missing files\n",
    "    missing_files = []\n",
    "    \n",
    "    # Iterate through each plant class in the dataset\n",
    "    for plant_class in os.listdir(dataset_dir):\n",
    "        class_dir = os.path.join(dataset_dir, plant_class)\n",
    "        \n",
    "        if os.path.isdir(class_dir):  # Check if it's a directory\n",
    "            for filename in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                \n",
    "                # Check if the file exists\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    missing_files.append(file_path)  # Add missing file to the list\n",
    "    \n",
    "    return missing_files\n",
    "\n",
    "# Check the training dataset\n",
    "print(\"Checking training dataset...\")\n",
    "missing_train_files = check_for_invalid_files(TRAIN_DIR)\n",
    "\n",
    "# Check the validation dataset\n",
    "print(\"\\nChecking validation dataset...\")\n",
    "missing_valid_files = check_for_invalid_files(VALIDATION_DIR)\n",
    "\n",
    "# Report results\n",
    "if missing_train_files:\n",
    "    print(f\"\\nTotal missing files in training dataset: {len(missing_train_files)}\")\n",
    "else:\n",
    "    print(\"\\nNo missing files in training dataset.\")\n",
    "\n",
    "if missing_valid_files:\n",
    "    print(f\"\\nTotal missing files in validation dataset: {len(missing_valid_files)}\")\n",
    "else:\n",
    "    print(\"\\nNo missing files in validation dataset.\")\n",
    "\n",
    "print(\"\\nFinished checking both datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6608de1-7524-4ae9-a911-ec50ae803865",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 6, figsize=(14, 3))\n",
    "fig.suptitle('Plant Disease Images', fontsize=16)\n",
    "\n",
    "# Loop through the first 6 classes in Diseases_classes and plot an image from each\n",
    "for ii, disease in enumerate(Diseases_classes[:6]):\n",
    "    # Set the directory for the current disease class\n",
    "    dir = f'{TRAIN_DIR}/{disease}'\n",
    "    # Load the first image in the directory\n",
    "    img = tf.keras.preprocessing.image.load_img(dir + '/' + os.listdir(dir)[0])\n",
    "    # Display the image in the subplot\n",
    "    axes[ii].imshow(img)\n",
    "    axes[ii].set_title(f'{disease}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0c21c-e765-4760-a8cd-3ab256ad1291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(60, 60), dpi=200)\n",
    "cnt = 0\n",
    "plant_names = []\n",
    "tot_images = 0\n",
    "\n",
    "for i in Diseases_classes:\n",
    "    cnt += 1\n",
    "    plant_names.append(i)\n",
    "    plt.subplot(7, 7, cnt)\n",
    "    \n",
    "    # Get only valid image files by filtering out hidden folders like .ipynb_checkpoints\n",
    "    image_path = [f for f in os.listdir(os.path.join(TRAIN_DIR, i)) if not f.startswith('.')]\n",
    "    \n",
    "    print(\"The Number of Images in \" + i + \":\", len(image_path))\n",
    "    tot_images += len(image_path)\n",
    "    \n",
    "    # Load and display the first image if there are any images in the folder\n",
    "    if image_path:\n",
    "        img_show = plt.imread(os.path.join(TRAIN_DIR, i, image_path[0]))\n",
    "        plt.imshow(img_show)\n",
    "        plt.xlabel(i, fontsize=30)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    \n",
    "print(\"\\nTotal Number of Images in Directory:\", tot_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e66af-6154-4e65-8291-f90265ef2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_datasets():\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory( \n",
    "        directory=TRAIN_DIR,\n",
    "        batch_size=32,\n",
    "        image_size=(256, 256),\n",
    "\t\tlabel_mode= \"int\",\n",
    "        color_mode= \"rgb\",\n",
    "    ) \n",
    "    \n",
    "    validation_dataset = tf.keras.utils.image_dataset_from_directory( \n",
    "        directory=VALIDATION_DIR,\n",
    "        batch_size=32,\n",
    "        image_size=(256, 256),\n",
    "\t\tlabel_mode=\"int\",\n",
    "        color_mode= \"rgb\",\n",
    "    ) \n",
    "    \n",
    "    return train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8751f36-b136-4956-9e69-fab76fe69245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = train_val_datasets()\n",
    "print(f\"Images of train dataset have shape: {train_dataset.element_spec[0].shape}\")\n",
    "print(f\"Labels of train dataset have shape: {train_dataset.element_spec[1].shape}\")\n",
    "print(f\"Images of validation dataset have shape: {validation_dataset.element_spec[0].shape}\")\n",
    "print(f\"Labels of validation dataset have shape: {validation_dataset.element_spec[1].shape}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7de7b1-9a6d-48ab-9274-ee46b05e32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255, input_shape=(256, 256, 3)),\n",
    "        \n",
    "        # Conv Block 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # Conv Block 4 (Tambahan)\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(38, activation='softmax')  # 38 class output\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5745972-d4c2-4b0b-8178-a024139467e8",
   "metadata": {},
   "source": [
    "## Jalankan Salah Satu Aja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e1889-d65a-48c5-866c-b765f40af8fb",
   "metadata": {},
   "source": [
    "### Untuk Training Dari Awal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5151f948-0214-49d3-bbc8-92c0de24b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf23781-b762-498d-9391-e072aa81f11d",
   "metadata": {},
   "source": [
    "### Untuk Training Import Model yang Udah Ada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbc57f-a004-4b2c-ae8d-f96628f4ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('plant_disease_model5.h5')\n",
    "model.load_weights('plant_disease_weights5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639160a1-12e9-42fe-b3e1-2cc305f91cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Input shape: {model.input_shape}')\n",
    "print(f'Output shape: {model.output_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0e9db-87d5-4c85-ad46-e2eb9a243e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c41cdf-3322-4c47-ad77-df66b0adf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dataset.take(1):\n",
    "\texample_batch_images = images\n",
    "\texample_batch_labels = labels\n",
    "\t\n",
    "try:\n",
    "\tmodel.evaluate(example_batch_images, example_batch_labels, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function, last layer and label_mode are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = model.predict(example_batch_images, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ff55b-83f7-4431-8474-023f4e5cdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=3, restore_best_weights=True, monitor='val_loss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da4006-81de-401c-af73-5d3294434c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=4,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f06ea-441e-4ea7-ba40-30cc314832d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uploader = widgets.FileUpload(accept=\"image/*\", multiple=True, description='Upload Gambar', button_style='primary')\n",
    "out = widgets.Output()\n",
    "display(uploader)\n",
    "display(out)\n",
    "\n",
    "def file_predict(filename, file, out):\n",
    "    image = tf.keras.utils.load_img(file, target_size=(256, 256))\n",
    "    image = tf.keras.utils.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    prediction = model.predict(image, verbose=0)[0]\n",
    "    confidence_score = np.max(prediction) * 100\n",
    "    \n",
    "    with out:\n",
    "\n",
    "        print(f'\\nmodel output: {prediction}')\n",
    "        \n",
    "        prediction_index = np.argmax(prediction)\n",
    "        \n",
    "        dataset = tf.keras.utils.image_dataset_from_directory(TRAIN_DIR)\n",
    "\n",
    "        # Ambil nama kelas\n",
    "        classes = dataset.class_names\n",
    "        \n",
    "        predicted_class = classes[prediction_index]\n",
    "        \n",
    "        print(f'{filename} is predicted as {predicted_class} with a confidence score of {confidence_score:.2f}%')\n",
    "\n",
    "\n",
    "def on_upload_change(change):\n",
    "    items = change.new\n",
    "    for item in items: # Loop if there is more than one file uploaded  \n",
    "        file_jpgdata = BytesIO(item.content)\n",
    "        file_predict(item.name, file_jpgdata, out)\n",
    "        \n",
    "uploader.observe(on_upload_change, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22fe9b2-8aa6-418e-89d0-680822af23f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model\n",
    "model.save('plant_disease_model6.h5')\n",
    "\n",
    "# Jika ada weights/bobot tertentu yang ingin disimpan\n",
    "model.save_weights('plant_disease_weights6.h5')\n",
    "\n",
    "import pickle\n",
    "with open('training_history6.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34224a2-7474-4d4f-b307-a76bbba98a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "  '''Plots the training and validation loss and accuracy from a history object'''\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(acc))\n",
    "\n",
    "  fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
    "  ax[0].plot(epochs, acc, 'b-', label='Training accuracy')\n",
    "  ax[0].plot(epochs, val_acc, 'r-', label='Validation accuracy')\n",
    "  ax[0].set_title('Training and validation accuracy')\n",
    "  ax[0].set_xlabel('epochs')\n",
    "  ax[0].set_ylabel('accuracy')\n",
    "  ax[0].legend()\n",
    "\n",
    "  ax[1].plot(epochs, loss, 'b-', label='Training Loss')\n",
    "  ax[1].plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "  ax[1].set_title('Training and validation loss')\n",
    "  ax[1].set_xlabel('epochs')\n",
    "  ax[1].set_ylabel('loss')\n",
    "  ax[1].legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_loss_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3d635-ab97-49ed-a494-8c49790e896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Simpan history ke file JSON\n",
    "with open('training_history5.json', 'w') as file:\n",
    "    json.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19caa816-ba3e-482c-8964-e16ed5ef793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Untuk memuat kembali history dari file\n",
    "with open('training_history5.json', 'r') as file:\n",
    "    history = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e9614-64ac-4e5f-8e44-ee0c9655f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('training_history5.pkl', 'rb') as file:\n",
    "    history = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a91ec-557c-4608-9c1f-fa7d2ea1aa21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(validation_dataset, verbose=2)\n",
    "print(\"Restored model, accuracy: {:5.2f}%\".format(100 * accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871eca77-25d7-4b67-b7a7-c72e911a1755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
